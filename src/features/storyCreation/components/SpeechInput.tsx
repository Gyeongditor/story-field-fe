import React, { useState, useEffect } from 'react';
import { Alert } from 'react-native';
import styled from '@emotion/native';
import { Audio } from 'expo-av';
import Constants from 'expo-constants';
import * as FileSystem from 'expo-file-system';
import { convertSpeechToText } from '../../../shared/lib/speechToText';

const Container = styled.View`
  flex: 1;
  align-items: center;
  justify-content: center;
  padding: 24px;
`;

const Content = styled.View`
  align-items: center;
  width: 100%;
  max-width: 320px;
`;

const Instruction = styled.Text`
  font-size: 16px;
  color: #6b7280;
  text-align: center;
  line-height: 24px;
  margin-bottom: 40px;
`;

const MicContainer = styled.View`
  align-items: center;
  margin-bottom: 40px;
`;

const MicButton = styled.TouchableOpacity<{ recording: boolean }>`
  width: 120px;
  height: 120px;
  border-radius: 60px;
  background-color: ${props => props.recording ? '#ef4444' : '#3b82f6'};
  align-items: center;
  justify-content: center;
  shadow-color: #000;
  shadow-offset: 0px 4px;
  shadow-opacity: 0.1;
  shadow-radius: 8px;
  elevation: 4;
`;

const MicIcon = styled.Text`
  font-size: 48px;
  color: #ffffff;
`;

const StatusText = styled.Text<{ recording: boolean }>`
  font-size: 18px;
  font-weight: 500;
  color: ${props => props.recording ? '#ef4444' : '#6b7280'};
  text-align: center;
`;

const Footer = styled.View`
  flex-direction: row;
  gap: 16px;
  width: 100%;
`;

const GhostButton = styled.TouchableOpacity`
  flex: 1;
  padding: 16px;
  border-radius: 8px;
  border: 1px solid #e5e7eb;
  align-items: center;
`;

const GhostText = styled.Text`
  font-size: 16px;
  font-weight: 500;
  color: #6b7280;
`;

const PrimaryButton = styled.TouchableOpacity<{ disabled?: boolean }>`
  flex: 2;
  padding: 16px;
  border-radius: 8px;
  background-color: ${props => props.disabled ? '#d1d5db' : '#3b82f6'};
  align-items: center;
`;

const PrimaryText = styled.Text<{ disabled?: boolean }>`
  font-size: 16px;
  font-weight: 600;
  color: ${props => props.disabled ? '#9ca3af' : '#ffffff'};
`;

interface SpeechInputProps {
  onCancel: () => void;
  onComplete: (text: string) => void;
}

export default function SpeechInput({ onCancel, onComplete }: SpeechInputProps) {
  const [recording, setRecording] = useState<Audio.Recording | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [recordingUri, setRecordingUri] = useState<string | null>(null);
  const [hasRecorded, setHasRecorded] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);

  useEffect(() => {
    return () => {
      if (recording) {
        recording.stopAndUnloadAsync();
      }
    };
  }, [recording]);

  const requestPermissions = async () => {
    const { status } = await Audio.requestPermissionsAsync();
    if (status !== 'granted') {
      Alert.alert('ê¶Œí•œ í•„ìš”', 'ìŒì„± ë…¹ìŒì„ ìœ„í•´ ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
      return false;
    }
    return true;
  };

  const startRecording = async () => {
    try {
      const hasPermission = await requestPermissions();
      if (!hasPermission) return;

      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
      });

      const { recording: newRecording } = await Audio.Recording.createAsync({
        android: {
          extension: '.wav',
          outputFormat: Audio.AndroidOutputFormat.DEFAULT,
          audioEncoder: Audio.AndroidAudioEncoder.DEFAULT,
          sampleRate: 16000,
          numberOfChannels: 1,
          bitRate: 128000,
        },
        ios: {
          extension: '.wav',
          audioQuality: Audio.IOSAudioQuality.HIGH,
          sampleRate: 16000,
          numberOfChannels: 1,
          bitRate: 128000,
          linearPCMBitDepth: 16,
          linearPCMIsBigEndian: false,
          linearPCMIsFloat: false,
        },
        web: {
          mimeType: 'audio/webm;codecs=opus',
          bitsPerSecond: 128000,
        },
      });

      setRecording(newRecording);
      setIsRecording(true);
      console.log('ğŸ¤ ë…¹ìŒ ì‹œì‘');
    } catch (error) {
      console.error('ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:', error);
      Alert.alert('ë…¹ìŒ ì˜¤ë¥˜', 'ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  };

  const stopRecording = async () => {
    if (!recording) return;

    try {
      setIsRecording(false);
      await recording.stopAndUnloadAsync();
      
      const uri = recording.getURI();
      if (uri) {
        // recordings í´ë” ìƒì„±
        const recordingsDir = `${FileSystem.documentDirectory}recordings/`;
        const dirInfo = await FileSystem.getInfoAsync(recordingsDir);
        if (!dirInfo.exists) {
          await FileSystem.makeDirectoryAsync(recordingsDir, { intermediates: true });
        }

        // íŒŒì¼ ë³µì‚¬
        const fileName = `recording-${Date.now()}.wav`;
        const finalPath = `${recordingsDir}${fileName}`;
        await FileSystem.copyAsync({
          from: uri,
          to: finalPath
        });

        setRecordingUri(finalPath);
        setHasRecorded(true);
        console.log('ğŸ“ ë…¹ìŒ ì™„ë£Œ! íŒŒì¼ ìœ„ì¹˜:', finalPath);
      }
      
      setRecording(null);
    } catch (error) {
      console.error('ë…¹ìŒ ì •ì§€ ì‹¤íŒ¨:', error);
      Alert.alert('ë…¹ìŒ ì˜¤ë¥˜', 'ë…¹ìŒì„ ì •ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  };

  const handleMicPress = () => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  };

  const handleUseRecording = async () => {
    if (!recordingUri) {
      Alert.alert('ì˜¤ë¥˜', 'ë…¹ìŒëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.');
      return;
    }

    setIsProcessing(true);
    try {
      console.log('ğŸ”„ STT ë³€í™˜ ì‹œì‘:', recordingUri);
      const result = await convertSpeechToText(recordingUri);
      
      if (result.success && result.text) {
        console.log('âœ… STT ì„±ê³µ:', result.text);
        onComplete(result.text);
      } else {
        console.error('âŒ STT ì‹¤íŒ¨:', result.error);
        Alert.alert('ë³€í™˜ ì‹¤íŒ¨', `ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\nì˜¤ë¥˜: ${result.error}`);
      }
    } catch (error: any) {
      console.error('ğŸ”¥ STT ì²˜ë¦¬ ì˜¤ë¥˜:', error);
      Alert.alert('ì²˜ë¦¬ ì˜¤ë¥˜', error.message);
    } finally {
      setIsProcessing(false);
    }
  };

  const retryRecording = () => {
    setRecordingUri(null);
    setHasRecorded(false);
  };

  return (
    <Container>
      <Content>
        <Instruction>
          {hasRecorded 
            ? 'ë…¹ìŒì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°"ë¥¼ ëˆŒëŸ¬ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.'
            : 'ìì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.\në§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒì„ ì‹œì‘í•˜ì„¸ìš”.'
          }
        </Instruction>

        <MicContainer>
          <MicButton recording={isRecording} onPress={handleMicPress}>
            <MicIcon>{isRecording ? 'â¹' : 'ğŸ¤'}</MicIcon>
          </MicButton>
        </MicContainer>

        <StatusText recording={isRecording}>
          {isRecording ? 'ë…¹ìŒ ì¤‘...' : hasRecorded ? 'ë…¹ìŒ ì™„ë£Œ' : 'ë…¹ìŒ ëŒ€ê¸°'}
        </StatusText>
      </Content>

      <Footer>
        <GhostButton onPress={hasRecorded ? retryRecording : onCancel}>
          <GhostText>{hasRecorded ? 'ë‹¤ì‹œ ë…¹ìŒ' : 'ì·¨ì†Œ'}</GhostText>
        </GhostButton>
        <PrimaryButton disabled={!hasRecorded || isProcessing} onPress={handleUseRecording}>
          <PrimaryText disabled={!hasRecorded || isProcessing}>
            {isProcessing ? 'STT ë³€í™˜ ì¤‘...' : 'í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°'}
          </PrimaryText>
        </PrimaryButton>
      </Footer>
    </Container>
  );
}
